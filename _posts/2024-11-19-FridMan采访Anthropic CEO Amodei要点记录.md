---
title: FridMan采访Anthropic CEO Amodei要点记录
date: 2024-11-19 13:08:42 +0800
categories: 技术
tags:
  - AI
layout: single
toc: "true"
toc_label: 目录
toc_icon: list
toc_sticky: "true"
sidebar:
  nav: docs
comments: "true"
---
## Scaling laws

1. Dario Amodei现在认为，尽管未来总是不确定的，但有理由相信扩展将继续，尽管我们还没有在理论上完全解释这种“魔法”。
2. Dario Amodei 提到了网络规模、训练时间和数据量的线性扩展。他比喻说，这就像化学反应，需要线性地增加所有三种成分，如果只增加其中一个而其他不增加，反应就会停止。但如果所有成分都按比例增加，反应就可以继续进行。
3. Dario Amodei 还提到，他们已经在除语言之外的许多领域记录了扩展法则，包括图像、视频、文本到图像、图像到文本和数学等领域。他指出，这些领域都显示出相同的模式。他还提到了网络大小和数据大小为什么会导致更智能的模型的直觉。他以物理学家的身份回顾了物理学中的一些概念，比如1/F噪声和1/X分布，这些概念描述了自然过程中不同尺度的分布。他推测，语言中的模式也遵循类似的长尾分布，随着网络的增大，模型能够捕捉到越来越多的分布，从而提高了模型的预测能力和表现。
## Limits of LLM scaling

1. Dario Amodei 认为没有人确切知道真实世界的复杂性和可学习内容的极限。他直觉上认为，在人类水平以下没有上限。如果我们继续扩展这些模型并开发新的训练方法，至少可以达到人类水平。
2. 在生物学等领域，人类对复杂性的理解仍然有限，因此AI有潜力变得更智能。而在材料科学或解决人际冲突等领域，可能存在难以逾越的障碍。
3. 某些领域的进展可能受限于人类机构和官僚体制。例如，尽管技术发展迅速，但药物开发仍需经过临床试验等程序。
4. Dario 讨论了计算资源可能成为限制因素，尤其是在构建更大数据中心的成本方面。但他也提到，目前有很多决心在本国建立计算能力，预计到2027年，我们将拥有价值千亿美元的计算集群
5. Scaling Law的限制可能会来源于数据缺乏和数据质量，但是可以用AI生成的数据或者强化学习的数据（本质也是一种生成数据）来弥补。
6. 如果模型扩展后停止改进，可能需要发明新的架构或优化方法。历史上曾有模型因数值稳定性问题而看似达到瓶颈，但找到正确的解决方法后，它们并没有真正达到极限。
7. Amodei目前并没有看到Scaling Law遇到瓶颈的证据，仍需要建设更大的数据中心，可能是目前规模的100倍。
## Competition with OpenAI, Google, xAI, Meta

1. Anthropic 的使命是推动整个领域朝着积极的方向发展，他们提到了“Race to the Top”（向顶部赛跑）的理念，即通过树立榜样来推动其他参与者做正确的事情。Dario 举了 Anthropic 早期的例子，他们让联合创始人 Chris Olah 和团队专注于提高 AI 模型的可解释性，即使这在商业上没有直接应用。他们公开分享了研究成果，希望这样做能够提高模型的安全性。随着时间的推移，其他公司也开始关注可解释性，这减少了 Anthropic 的竞争优势，但 Dario 认为这对整个系统是有益的。他强调，Anthropic 需要不断创新，以保持其在行业中的领先地位。
2. Dario 还提到了 Anthropic 对 AI 安全的承诺，他们正在尝试以一种严谨的、非敷衍的方式来处理 AI 安全问题。他们发现，尽管 AI 模型并不是为我们理解而设计的，但当我们深入研究这些系统时，我们能够发现一些令人惊讶的有趣现象。通过 MEC 和 TERP 方法，他们能够探索大型神经网络的美丽本质，并发现模型中的某些方向与清晰的概念相对应。
3. Dario 还提到了他们进行的一个实验，即 Golden Gate Bridge Claude，通过调整模型的行为，使其对 Golden Gate Bridge 有强迫性的专注，这使得模型在情感上显得更像人类。这个实验展示了通过干预模型的行为，可以使其表现出更强烈的个性和身份，就像一个对某事物有强烈兴趣的人一样。这种个性化的调整使得模型在与用户的互动中显得更加真实和有吸引力。
## Claude

1. 每一代新模型都会使用新数据，并且它们的“个性”会以开发者试图引导但不完全可控的方式发生变化。因此，并不是每一次模型更新都只是智能的提高，还可能包括其他方面的改进，以及一些无法测量的变化。
2. 开发者试图在每次模型更新中改进性能，但这是一个不精确的科学，模型的举止和个性更多地像是一门艺术而不是纯粹的科学。

## Opus 3.5

1. 在开发新模型时，团队会同时改进所有方面，包括预训练和后训练。
2. 旧模型的偏好数据有时用于新模型，尽管在新模型上训练的数据表现更好。
3. 后训练过程变得越来越复杂，不仅仅使用RLHF（从人类反馈中的强化学习），还包括模型自我对抗训练等其他方法。
## Sonnet 3.5

1. Dario Amodei 指出，与之前的代码模型相比，Sonnet 3.5是第一个真正节省了经验丰富的工程师时间的模型。这些工程师之前认为以前的模型只对初学者有用，但Sonnet 3.5在某些情况下帮助他们完成了本来需要花费数小时的任务。
2. Sonnet 3.5 不仅仅受益于预训练，也来自于后训练过程的改进，以及精细化的评估方法。
3. Dario Amodei 强调，如果模型在SWE-bench上的表现达到100%，并且不是过度训练或专门为该基准测试优化的，那么这将代表编程能力的真正提升。他预测，如果模型能够达到90%到95%的表现，那么它将能够自主完成大部分软件工程任务。

## Claude 4.0
1. 模型的命名和参数定义很难，因为每个模型都有很多的属性，甚至是个性，就和人一样。和传统软件的定义和命名不一样。
2. 模型有许多属性并未在基准测试中反映出来，例如模型的礼貌程度、反应性、提问能力、个性（温暖或冷漠）、以及是否乏味或独特等。这些属性对于用户体验至关重要。


## Criticism of Claude
1. 有时候觉得模型变笨，是由于A/B测试或者是系统提示词修改
2. 人们对于 Claude 变笨的感觉可能与模型的复杂性和多面性有关。模型对不同的提问方式可能会有不同的反应，因此与模型的互动方式的微小变化可能会导致截然不同的结果。他还提到，人们对新模型的兴奋感会随着时间的推移而减少，随着对模型局限性的认识增加，可能会产生模型性能下降的感觉。
3. 模型的行为很难控制和引导，比如它的过分道歉行为。需要做了各种情况的均衡。每次做一些调整，都需要做大量的回归测试和评估。(我猜测，这就是Anthropic一直要做模型可解释的其中一个原因。)
## AI Safety Levels
1. AI安全等级（AI Safety Levels，简称ASL）是一套用于评估和分类AI系统安全性的框架，旨在确定AI系统在自主性和潜在滥用风险方面的能力水平。以下是ASL的几个分类的总结，目前处于ASL2，正在部署ASL3级别的安全措施，今年年底或明年能会触发ASL3级别
2. AI安全等级（AI Safety Levels，简称ASL）是一套用于评估和分类AI系统安全性的框架，旨在确定AI系统在自主性和潜在滥用风险方面的能力水平。以下是ASL的几个分类的总结

	- **ASL-1**：这一级别的系统明显不具有自主性或滥用的风险。例如，一个国际象棋机器人，如深蓝（Deep Blue），它被设计仅用于下棋，不会用于其他任何目的，如网络攻击或控制世界。
	- **ASL-2**：这是指当前的AI系统，我们认为这些系统还不够智能，无法自主复制或执行许多任务，也无法提供超出谷歌搜索能力的关于化学、生物、放射性和核（CBRN）风险和如何制造CBRN武器的有意义信息。
	- **ASL-3**：在这个级别，模型足够强大，可以增强非国家行为者的能力。国家行为者已经能够以高度熟练的方式执行许多危险和破坏性的活动，而非国家行为者则不具备这种能力。当我们达到ASL-3时，我们将采取特别的安全预防措施，以防止模型被非国家行为者窃取和滥用。
	- **ASL-4**：在这个级别，这些模型可以增强已经知识渊博的国家行为者的能力，或者成为此类风险的主要来源。如果有人想要从事此类风险活动，主要方式是通过模型来实现。在自主性方面，ASL-4涉及到AI模型在AI研究能力上的一定程度的加速。
	- **ASL-5**：在这个级别，我们将拥有真正能够超越人类在执行任何任务上的能力的模型。
## Computer User
1. computer user并没有对模型做特殊的训练。
2. 如果有一个强大的预训练模型，你几乎可以完成任何任务，因为模型已经具备了所需的所有表示能力。通过循环给模型提供屏幕截图并告诉它点击哪里，模型可以完成类似视频交互的任务，比如填写电子表格、与网站交互或打开各种程序。

## 为什么离开openai

2. 离开openai的原因应该是某种理念不合，证明自己的理念。这个分歧应该是安全方面和可控、可解释方面。

## Hiring a great team
1. 人才密度要比人才数量重要。100全部是高质量人才的团队要比1000个中有200个高质量人才的团队要好。
2. Anthropic招聘了很多理论物理学家
3. 开放心态和新眼光看待事物，对人工智能研究员最重要。
4. 建议人们去玩模型，通过实践来获得经验是很有必要的。顺应做一些新事物、朝新的方向思考的理念，有很多事情尚未被探索。建议研究机制可解释性、长期学习和长期任务、评估方法、多智能体系统。


## Post-training
1. Anthropic优势通常来自于基础设施的改进、数据质量的提升和方法的结合，而不是单一的“魔法方法”。
2. 现代后训练方法多样：包括监督式微调、RLHF、宪法人工智能和合成数据的使用。
3. 不要强加你自己对模型应该如何学习的想法。
4. RLHF之所以有效，是因为它能够在人类和模型之间架起桥梁，使模型能够更好地执行人类想要的任务。Dario Amodei 认为，RLHF并没有使模型变得更智能，而是改善了模型与人类的沟通。
5. 目前，预训练是成本的主要部分，但未来后训练可能会成为成本的主要部分。Dario Amodei 预测，随着后训练的重要性增加，成本结构可能会发生变化。
6. Dario Amodei 认为，由于人类无法被大规模扩展以保证高质量输出，未来后训练中依赖人类的部分可能会转向规模化的监督方法，如辩论或迭代放大等。
## Machines of Loving Grace

1. Dario认为，即使是高度智能的AI也难以预测复杂系统的结果，例如经济、生物分子的相互作用、人类制度。
2. AI发展会受到物理世界中的局限性的影响，如硬件生产的时间和复杂性，以及人类机构挑战和人类社会法律、道德的适应。
3. 历史上的生产力增长，如计算机革命和互联网革命，以及这些技术带来的生产力增长往往低于预期，因为企业制度的拖慢。Dario 讨论了技术在全球范围内推广的缓慢速度，特别是在贫困地区。
4. Dario 对未来持有谨慎的乐观态度，他认为AI技术的影响可能在5到10年内显现，而不是50或100年。他强调了人类系统的变化通常比预期的要慢，但也强调了变革最终会发生。

## AGI timeline
1.  powerful AI可能会在2026年或2027年出现、
2. 在生物学方面，AI将成为 CRISPR 类型技术的发明者。现在的情况将会翻转，人会变为助手，AI将指挥人类去操作一些实验室设备。
3. AI尽管不能替代临床试验，不能够进行模拟，但应该会大大提升效率，缩短时间。

## 编程
   
1. 编程技能与人工智能的构建密切相关。Dario 认为，一个技能与构建人工智能的人越远，它被人工智能颠覆所需的时间就越长。他相信人工智能最终会颠覆农业等其他领域，但由于这些领域与人工智能的构建者相距较远，因此这种颠覆需要更长的时间。相反，编程是许多在 Anthropic 以及其他公司工作的员工的主要技能，因此这一变化将会很快发生。
2. 模型可以在训练和应用过程中形成闭环。Dario 解释说，模型能够编写代码，然后运行代码并查看结果，再将结果反馈回来。这种能力是硬件和生物学等其他领域所不具备的，因此模型在编程方面的进步将会非常迅速。他提到，在今年一月份，模型在典型现实世界编程任务中的表现只有3%，而到了十月份，这一数字已经上升到了50%。Dario 认为，我们正处于一个S曲线上，随着接近100%的极限，增长速度将会开始放缓。但他预测，在接下来的10个月左右，模型的表现可能会非常接近90%。他预计，到2026年或2027年，模型在编程方面的能力将会非常接近人类水平。
3. 他强调了比较优势的概念，即当AI能够完成80%的编码工作时，人类在剩余20%的任务中的作用将变得更加重要，这些任务可能涉及更高层次的系统设计、架构评估、用户体验（UX）设计等方面。Dario 承认，总有一天AI可能会在所有领域超越人类，到那时，比较优势的逻辑将不再适用。他强调，这是一个需要严肃对待的重大问题，需要人类集体思考如何应对。然而，他认为在短期内，甚至在中期内（2-4年），人类在编程领域仍将扮演重要角色，编程工作的性质会改变，但编程作为一种职业不会消失。

## 人生意义

1. 生命的意义：重要的不是结果，而是过程中个人的选择、努力和与他人的互动。
2. 他认为这是比AI的自主风险或生活意义问题更严重的问题。他担心如何确保技术带来的公平世界能够惠及每个人，以及如何防止权力被滥用，特别是在独裁和专制体制中。
3. 大多数人们的生活虽然努力寻找意义，但很多人还在为生存而挣扎。如果能够将技术的好处分配到全球，那么人们的生活将得到极大改善，生活的意义对他们来说将变得更加重要。
4. 强大的AI有可能为每个人创造更多的意义，让每个人都能体验到以前没有人或极少数人能体验到的世界和经历。